{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Mike Brodie         <p>Sr. Data Scientist @ Ancestry</p> <p> <ul> <li>                  Lead Data Scientist on numerous high-impact projects involving Computer Vision, Natural Language Processing, and state-of-the-art Transformer models at scale.                   </li> <li>                  Significantly accelerated AIML-based record extraction at Ancestry, with my models producing 24 billion new historical records added to Ancestry in 2023 alone.                   </li> <li>                  Granted multiple patents for my contributions to the field, and have had the privilege of working for companies such as Adobe and Apple.                   </li> <li>                  In addition to my industry experience, I have also taught a machine learning course as adjunct faculty at BYU.                  </li> </ul> </p>"},{"location":"bart_for_summarization/","title":"BART","text":""},{"location":"bart_for_summarization/#1-introduction","title":"1. Introduction","text":"<p>Document summarization is the process of generating a condensed version of a document,  preserving its most important information. Automatic document summarization systems have  gained significant attention in recent years due to their potential applications in  various fields such as news aggregators, educational resources, and legal documents.</p> <p>One popular approach for document summarization is based on Transformers models. These models, such as BART (Bidirectional and Auto-Regressive Transformer), have shown  remarkable success in several NLP tasks. In this post, we'll explore BART and discuss how it can be used for document summarization.</p>"},{"location":"bart_for_summarization/#2-background","title":"2. Background","text":"<p>Transformers are a type of neural network architecture introduced in Attention Is All You Need by Vaswani et al. They replaced the Recurrent  Neural Networks (RNN) and Long Short-Term Memory (LSTM) models that dominated NLP tasks  before their arrival. Transformers introduced a novel self-attention mechanism, which  allows the model to focus on different parts of the input simultaneously.</p> <p>BART (Bidirectional and Auto-Regressive Transformer) is a neural machine translation  model that combines the strengths of encoder-decoder transformer architectures with  bidirectional encoders. It was developed by Facebook AI Research (FAIR) as an improvement  on the existing Seq2Seq model.</p>"},{"location":"bart_for_summarization/#3-bart-architecture","title":"3. BART Architecture","text":"<p>BART consists of an encoder and a decoder, both using transformer layers with  self-attention mechanisms. The main difference between BART and traditional transformer  models is that it uses a bidirectional encoder, which processes both forward and backward  pass information for context understanding. This allows the model to capture more  contextual information and improve overall translation quality.</p>"},{"location":"bart_for_summarization/#encoder","title":"Encoder","text":"<p>The encoder consists of multiple identical self-attention layers followed by  position-wise feed-forward networks (FFN). The self-attention mechanism allows the model  to learn relationships between all input tokens in a single forward pass.</p>"},{"location":"bart_for_summarization/#decoder","title":"Decoder","text":"<p>The decoder also follows a similar architecture, but it's designed to generate output  tokens conditioned on encoder outputs. It generates one token at a time based on the  previous generated tokens and the encoder outputs. The decoder uses teacher forcing  during training by providing ground truth tokens at each step. During inference, the  decoder uses its own previous predictions as inputs.</p>"},{"location":"bart_for_summarization/#masked-language-modeling-mlm","title":"Masked Language Modeling (MLM)","text":"<p>BART includes an additional masked language modeling objective that encourages the model  to learn rich contextualized representations of input words. The MLM loss is computed  based on the prediction of masked tokens in the input text using the encoder outputs as  context.</p>"},{"location":"bart_for_summarization/#4-how-does-bart-work-for-document-summarization","title":"4. How Does BART Work for Document Summarization?","text":"<p>BART learns to summarize documents by pretraining it on large datasets of text and their  corresponding summaries. During pretraining, the model is encouraged to generate summary  tokens based on encoder outputs and to minimize the loss between the generated summary and the ground truth summary.</p>"},{"location":"bart_for_summarization/#conclusion","title":"Conclusion","text":"<p>BART uses an auto-regressive decoding mechanism, which generates  translations sequentially, one word at a time. This approach -- while slower than a single forward-feed prediction -- helps in capturing the  statistical dependencies between words and improving the coherence of generated  translations.</p> <p>BART has been successfully applied to various tasks, including text summarization, machine translation, and dialogue response generation. It demonstrates better performance compared to other transformer-based models due to its ability to capture more contextual  information and generate more coherent translations.</p>"},{"location":"bart_for_summarization/#example-of-running-bart-trained-on-the-cnn-news-dataset","title":"Example of running BART (trained on the CNN News dataset)","text":"<pre><code>from transformers import BartForConditionalGeneration, BartTokenizer\n# Load the pre-trained BART model and tokenizer\nmodel_name = \"facebook/bart-large-cnn\"\nmodel = BartForConditionalGeneration.from_pretrained(model_name)\ntokenizer = BartTokenizer.from_pretrained(model_name)\n# Input texts as a list\ninput_texts = [\n\"\"\"Database administrators (DBAs) play a crucial role in managing,\n    maintaining and optimizing a database system to ensure data availability, performance, and reliability.\"\"\",\n\"\"\"However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases).\"\"\"\n]\ninput_text\n# Encode the input texts\ninput_ids = tokenizer.batch_encode_plus(input_texts, return_tensors=\"pt\", max_length=1024, truncation=True, pad_to_max_length=True)\n# Generate summaries\nsummary_ids = model.generate(input_ids[\"input_ids\"], max_length=150, min_length=30, length_penalty=2.0, num_beams=4, temperature=1.0, early_stopping=True, do_sample=True)\n# Decode and print the summaries\nsummaries = [tokenizer.decode(summary_id, skip_special_tokens=True) for summary_id in summary_ids]\nprint(summaries)\n</code></pre>"},{"location":"bert/","title":"BERT: Bidirectional Encoder Representations from Transformers","text":""},{"location":"bert/#overview","title":"Overview","text":"<p>BERT, or Bidirectional Encoder Representations from Transformers, is a groundbreaking NLP model developed by Google. One of the key advantages of BERT is its ability to understand context and meaning in both directions, forward and backward, through a self-supervised training process.</p>"},{"location":"bert/#key-points","title":"Key Points","text":"<ul> <li> <p>BERT achieves this advanced understanding of language by pre-training on massive amounts of unlabeled text data from sources like Wikipedia and books corpus. This allows it to capture the context and relationships between words in a sentence, rather than focusing solely on individual word meanings.</p> </li> <li> <p>The model is based on Transformer architecture, which was originally proposed for machine translation tasks. BERT uses a self-attention mechanism, which allows it to consider all possible word relationships and generate highly accurate contextualized representations of words. This makes the model more robust in various language understanding tasks, such as question answering, sentiment analysis, and natural language inference.</p> </li> <li> <p>BERT's performance has significantly outperformed previous state-of-the-art models on various benchmarking datasets, leading to its widespread adoption among researchers and developers. Due to this success, Google open-sourced BERT, making it accessible for further development and experimentation in the research community.</p> </li> <li> <p>BERT is a powerful and innovative natural language processing model that has revolutionized the field with its ability to understand context and meaning in both forward and backward directions. Its impact is evident through its continued use by researchers even years after its initial release.</p> </li> </ul>"},{"location":"bert/#discussion","title":"Discussion","text":"<p>One of the key features of BERT is its ability to capture the context and relationships between words in a sentence, rather than focusing solely on individual word meanings. This is achieved through a process of pre-training on massive amounts of unlabeled text data from sources like Wikipedia and books corpus. By analyzing this data, BERT is able to learn the patterns and relationships between words, allowing it to generate contextualized representations of words.</p> <p>For example, consider the sentence \"The cat sat on the mat.\" Without any context, the word \"cat\" could refer to any type of feline, but with the context of the sentence, we know that it is referring to a specific cat that is sitting on a mat. BERT is able to capture this context and relationship between the words in the sentence, allowing it to generate a more accurate representation of the sentence as a whole.</p> <p>In contrast, traditional word-based models such as word2vec or GloVe, which only consider the individual word meanings, would generate embeddings for \"cat\" and \"mat\" that do not capture the relationship between them in the context of the sentence. This can lead to less accurate predictions and poorer performance on tasks such as language translation or sentiment analysis. BERT's ability to capture context and relationships between words is a key factor in its success and has led to significant improvements across a wide range of NLP tasks.</p>"},{"location":"bert/#self-attention","title":"Self-attention","text":"<p>Self-attention is a mechanism in deep learning that allows a model to attend to different parts of an input sequence when making predictions. It is a way for the model to weigh the importance of different features in the input and use that information to make more accurate predictions.</p> <p>Intuitively, self-attention can be thought of as a way for the model to \"focus\" on certain parts of the input when making predictions. For example, if the input is a sequence of words, self-attention can allow the model to focus on certain words that are more important for making a prediction, rather than considering the entire sequence equally.</p> <p>Self-attention is implemented using a set of learnable weights that are used to compute a set of attention scores. These attention scores represent the degree to which each feature in the input is important for making a prediction. The attention scores are then used to weight the input features, so that more important features are given more weight and less important features are given less weight. This is especially useful for tasks like machine translation, where the model needs to understand the context of an entire sentence rather than individual words. Self-attention has also been used in computer vision tasks, where it can help the model to focus on certain parts of an image that are more important for making a prediction.</p> <p>Mathematically, Self-Attention can be represented as:</p> <p> </p> <p>where Q, K, V are learnable matrices for projections of queries (Q), keys (K) and values (V). </p> <p>The attention score between a query and each key in the sequence is calculated using a dot product, and then normalized through the softmax function to get the weight for each value vector.</p> <p>Let's break it down step-by-step: </p> <ol> <li>Calculate Query Matrix:</li> </ol> <p> </p> <p>where x is the input sequence and W_q is a weight matrix for queries.</p> <ol> <li>Calculate Key Matrix:</li> </ol> <p> </p> <p>where W_k is a weight matrix for keys.</p> <ol> <li>Calculate Value Matrix:</li> </ol> <p> </p> <p>where W_v is a weight matrix for values.</p> <p>We have now taken input x and compute 3 distinct linear transformations.</p> <ol> <li>Dot Product Attention:</li> </ol> <p>Attention scores are calculated as the dot product between Q and K, followed by a softmax function to obtain the weights for each value vector:</p> <p> </p> <p>where d_k is the dimensionality of key vectors.</p> <ol> <li>Weighted Sum of Value Matrix:</li> </ol> <p>Finally, the output of the self-attention mechanism is obtained by taking a weighted sum of value vectors using attention scores from step 4:</p> <p> </p> <p>Self-Attention is a powerful technique that allows models to capture context and relationships between parts of an input sequence. The mathematical implementation involves 1. Projecting inputs as queries, keys, and values through learnable weight matrices.  2. Dot product attention mechanism  3. Weighted sum over value vectors.</p>"},{"location":"bert/#summary","title":"Summary","text":"<p>BERT is particularly useful for NLP tasks where context is crucial. For example, in question answering, BERT can understand the context of a question and generate a accurate response, even when the question is phrased in a way that differs from the original text. Similarly, in sentiment analysis tasks, BERT can accurately identify the sentiment of a sentence, even when the sentiment is expressed in a subtle or nuanced way. BERT is highly versatile and can be adapted to a wide range of NLP tasks -- and has significantly impacted the way we approach language understanding in machine learning.</p>"},{"location":"bigfile/","title":"Order of Magnitude Faster Image Loading in Pytorch","text":"<p>This is my own verion of Microsoft's: <code>https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life/blob/master/Global/data/Load_Bigfile.py</code> <code>https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life/blob/master/Global/data/Create_Bigfile.py</code> You can follow their approach to adapt this to a PyTorch DataLoader, Tensorflow Dataloader etc. This reduced training time from days to hours when using millions of images (i.e. SOTA GPUs are not necessarily the training bottleneck. In some cases, loading the image data can provide an OOM speedup!)</p> <p>The idea is basically: - Read each image in your dataset - Use <code>struct</code> in Python to read the 'number of bytes for the title' and the 'number of bytes for the image data' - Similarly use <code>struct</code> to pack the data as (in binary):  <code>[length_of_image_1_title, image_1_title_data, length_of_image_1_data, image_1_data,...]</code></p>"},{"location":"bigfile/#setup","title":"Setup","text":"<pre><code>pip install click\n</code></pre>"},{"location":"bigfile/#the-code-python-mainpy-bg-yourbigfilebg","title":"The Code (python main.py --bg [yourbigfile.bg]","text":"<p><pre><code>Usage: bigfile.py folder [OPTIONS] COMMAND [ARGS]...\n\n  Load and Unload millions of files quickly\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  compress    Pack name and data for all files in PATH\n  decompress  Decompress BG files into PATH directory\n</code></pre> <pre><code>import click\nimport os\nimport struct\nfrom os.path import join, basename\n@click.group()\ndef cli():\n\"\"\"Bigfile Data Module\"\"\"\n@cli.group()\ndef folder():\n\"\"\"Load and Unload millions of files quickly\"\"\"\ndef scantree(path):\n\"\"\"Recursively yield DirEntry objects for given directory.\"\"\"\nfor entry in os.scandir(path):\nif entry.is_dir(follow_symlinks=False):\nyield from scantree(entry.path)  # see below for Python 2.x\nelse:\nyield entry\n@folder.command()\n@click.option(\"--path\", help=\"Path to files to dompress\")\n@click.option(\"--out\", help=\"Name of output file to create\")\ndef compress(path, out):\n\"\"\"Pack name and data for all files in PATH\"\"\"\nwith open(out,'wb') as f:\ncount = 0\nfor p in scantree(path):\nnm  = basename(p)\nnm_bytes = nm.encode('utf-8')\ntry:\nwith open(p, 'rb') as data:\ndata_bytes = data.read()\nexcept:\n# hidden .swp files will fail\ncontinue\nf.write(struct.pack('i', len(nm_bytes)))\nf.write(nm_bytes)\nf.write(struct.pack('i', len(data_bytes)))\nf.write(data_bytes)\ncount += 1\nif count % 10000 == 0:\nprint(f'Processed {count} files')\n@folder.command()\n@click.option(\"--bg\", help=\"Bigfile to decompress\")\n@click.option(\"--path\", help=\"Destination folder for decompressed files\")\ndef decompress(bg, path):\n\"\"\" Decompress BG files into PATH directory\"\"\"\nwith open(bg, 'rb') as f:\ntry:\nwhile True:\nnm_len = struct.unpack('i', f.read(4))[0]\nnm = f.read(nm_len).decode('utf-8')\nwith open( join(path, nm), 'wb') as out:\ndata_len = struct.unpack('i', f.read(4))[0]\ndata = f.read(data_len)\nout.write(data)\nexcept:\npass\nif __name__ == '__main__':\ncli()\n</code></pre></p>"},{"location":"broadcasting/","title":"Broadcasting Rules in PyTorch","text":"<p>In PyTorch, the broadcasting rules allow for element-wise operations between tensors of different shapes. In this case, the broadcasting rule is used to perform matrix multiplication between the tensors <code>a</code> and <code>b</code>.</p>"},{"location":"broadcasting/#example","title":"Example","text":"<p>Let's consider two tensors <code>a</code> and <code>b</code> with shapes <code>(10, 3, 4, 5)</code> and <code>(10, 3, 4, 4)</code> respectively. When we perform matrix multiplication between <code>a</code> and <code>b</code>, the resulting tensor will have the same shape as <code>a</code>. This is because the broadcasting rules allow for the missing dimensions in <code>b</code> to be \"filled in\" with ones. For example, when we multiply the first two dimensions of <code>a</code> and <code>b</code> (i.e., the batch and channel dimensions), the resulting tensor will have shape <code>(10, 3, 4, 5)</code>. This is because the batch dimension of <code>a</code> (i.e., the first dimension with size 10) is broadcasted to match the batch dimension of <code>b</code> (i.e., the first dimension with size 10), and the channel dimension of <code>a</code> (i.e., the second dimension with size 3) is broadcasted to match the channel dimension of <code>b</code> (i.e., the second dimension with size 3). The broadcasting rules work in a similar way for the other two dimensions of <code>a</code> and <code>b</code> (i.e., the height and width dimensions). The resulting tensor will have the same shape as <code>a</code>, with the missing dimensions in <code>b</code> being \"filled in\" with ones.</p>"},{"location":"broadcasting/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>The matrix multiplication between <code>a</code> and <code>b</code> is possible because of the broadcasting rules in PyTorch, which allow for element-wise operations between tensors of different shapes. The resulting tensor will have the same shape as <code>a</code>, with the missing dimensions in <code>b</code> being \"filled in\" with ones.</p>"},{"location":"broadcasting/#conclusion","title":"Conclusion","text":"<p>In PyTorch, the broadcasting rules allow for element-wise operations between tensors of different shapes. This makes it possible to perform matrix multiplication between tensors with different shapes, as long as the shapes are compatible according to the broadcasting rules.</p>"},{"location":"cnn_v_transformer/","title":"Transformers vs. Convolutional Neural Networks: A Comparative Analysis","text":""},{"location":"cnn_v_transformer/#introduction","title":"Introduction","text":"<p>Transformers and Convolutional Neural Networks (CNNs) are two distinct types of neural networks with different approaches to processing data. While both have their strengths and weaknesses, understanding the key differences between them can help us choose the most appropriate model for a given task.</p>"},{"location":"cnn_v_transformer/#architecture-and-information-handling","title":"Architecture and Information Handling","text":"<p>The main difference between transformers and CNNs lies in their architecture and how they handle information. Transformers focus on learning relationships between words or tokens, allowing them to understand the context and meaning within a sentence. They use attention mechanisms that enable the model to attend to relevant parts of the input sequence while ignoring irrelevant parts. This makes transformers particularly well-suited for tasks involving natural language processing (NLP), such as machine translation, text summarization, and question answering. On the other hand, CNNs are designed to process data with a regular grid structure, such as images or time series. They use convolutional layers that perform a specific type of operation called \"convolution\" on the input data. This allows them to extract features at different scales and identify patterns across the entire dataset. CNNs have shown great success in areas like computer vision and time series analysis.</p>"},{"location":"cnn_v_transformer/#long-range-dependencies","title":"Long-Range Dependencies","text":"<p>One of the main advantages of transformers over CNNs is their ability to capture long-range dependencies, making them more suitable for sequence modeling tasks. Transformers can understand the context of an input sequence better than CNNs, which are more focused on local patterns. This allows transformers to outperform CNNs in tasks like language translation and understanding, where maintaining context is crucial.</p>"},{"location":"cnn_v_transformer/#efficiency-and-scalability","title":"Efficiency and Scalability","text":"<p>However, CNNs have their advantages too, such as efficiency and scalability, which make them suitable for large-scale image or time series data processing. Additionally, the combination of both transformer and convolutional approaches (hybrid models) has shown promising results in various applications, leveraging the strengths of each architecture to tackle complex tasks more effectively.</p>"},{"location":"cnn_v_transformer/#conclusion","title":"Conclusion","text":"<p>In conclusion, both transformers and CNNs have their strengths and weaknesses, and the choice of which one to use depends on the specific task at hand. Transformers are better suited for tasks involving natural language processing, while CNNs excel in tasks involving regular grid structure data. However, the combination of both architectures in hybrid models can lead to better performance in complex tasks.</p>"},{"location":"consistency_encoder/","title":"Consistency encoder","text":"<pre><code>pip install git+https://github.com/openai/consistencydecoder.git\npip install diffusers\npip install git+https://github.com/huggingface/transformers\npip install accelerate\n</code></pre> <pre><code>import torch\nfrom diffusers import StableDiffusionPipeline\nfrom consistencydecoder import ConsistencyDecoder, save_image, load_image\n# encode with stable diffusion vae\npipe = StableDiffusionPipeline.from_pretrained(\n\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, device=\"cuda:0\"\n)\npipe.vae.cuda()\ndecoder_consistency = ConsistencyDecoder(device=\"cuda:0\") # Model size: 2.49 GB\nimage = load_image(\"bingo_halloween_2023.jpeg\", size=(256, 256), center_crop=True)\nlatent = pipe.vae.encode(image.half().cuda()).latent_dist.mean\nprint(latent.shape)\n# decode with gan\nsample_gan = pipe.vae.decode(latent).sample.detach()\nsave_image(sample_gan, \"gan.png\")\n# decode with vae\nsample_consistency = decoder_consistency(latent)\nsave_image(sample_consistency, \"con.png\")\nprint(latent.shape)\ntorch.Size([1, 4, 64, 64])\n</code></pre>"},{"location":"goleman/","title":"Book Review - What Makes a Leader","text":"<p>\"What Makes a Leader\" by Daniel Goleman emphasizes that leadership is not just about authority, but also about  emotional intelligence. The author argues that leaders who possess high levels of emotional intelligence are more  effective in their roles and have a significant impact on their organizations.</p> <p>Actionable takeaways from the book include:</p> <ol> <li> <p>Emotional intelligence: This refers to the capacity to be aware of, express, and manage our emotions and  understand the emotions of others. Goleman identifies five key components of emotional intelligence that are vital  for successful leadership: self-awareness, self-regulation, motivation, empathy, and social skills.</p> </li> <li> <p>Self-awareness: Successful leaders know themselves well, including their strengths, weaknesses, values, and goals. They can identify the impact of their emotions on their thinking and behavior and are open to feedback from others.</p> </li> <li> <p>Self-regulation: Leaders who can manage their emotional responses effectively are better at handling difficult  situations and making sound decisions under pressure. They remain calm during crises and practice active listening in group settings.</p> </li> <li> <p>Motivation: Highly effective leaders possess a strong internal motivation to achieve their goals and inspire  others to do the same. They have a clear vision for their team or organization and can articulate it persuasively,  which helps align the efforts of their followers.</p> </li> <li> <p>Empathy: A leader who can understand and share the feelings of others is better equipped to connect with their  team members on a deeper level. This fosters trust and loyalty within the team and allows for a more supportive work  environment.</p> </li> <li> <p>Social skills: Successful leaders are adept at building relationships, resolving conflicts, and working  effectively in teams. They can communicate clearly and inspire others to collaborate towards shared goals.</p> </li> </ol> <p>By developing these qualities, leaders can foster positive work environments, enhance their decision-making abilities, and drive success within their organizations.</p>"},{"location":"healthcare/","title":"Data Science and Healthcare: The Power of GenAI","text":"<p>In recent years, the field of data science has made significant strides in revolutionizing the healthcare industry. One of the most exciting developments in this area is the use of artificial intelligence (AI) to improve patient outcomes and streamline healthcare operations. In this blog post, we will explore the potential of GenAI in healthcare and how it is transforming the way we approach patient care.</p>"},{"location":"healthcare/#what-is-genai","title":"What is GenAI?","text":"<p>GenAI refers to the use of AI algorithms and techniques to analyze and interpret large amounts of genetic data. This data can come from a variety of sources, including DNA sequencing, microarray analysis, and epigenetic profiling. By analyzing this data, GenAI can help healthcare providers identify genetic risk factors for diseases, develop personalized treatment plans, and predict patient outcomes.</p>"},{"location":"healthcare/#the-benefits-of-genai-in-healthcare","title":"The Benefits of GenAI in Healthcare","text":"<p>One of the key benefits of GenAI in healthcare is its ability to improve patient outcomes. By analyzing genetic data, healthcare providers can identify patients who are at risk for certain diseases and develop personalized treatment plans that are tailored to their specific needs. This can lead to better health outcomes and a higher quality of life for patients. GenAI can also help healthcare providers streamline their operations and reduce costs. By automating many of the tasks involved in analyzing genetic data, GenAI can free up healthcare providers to focus on more complex and critical tasks. This can lead to more efficient and effective healthcare delivery, as well as reduced costs for both patients and healthcare providers.</p>"},{"location":"healthcare/#the-future-of-genai-in-healthcare","title":"The Future of GenAI in Healthcare","text":"<p>As the field of data science continues to evolve, the potential of GenAI in healthcare is only going to grow. In the coming years, we can expect to see even more advanced AI algorithms and techniques being developed to analyze and interpret genetic data. This will enable healthcare providers to develop even more personalized treatment plans and improve patient outcomes even further. In addition, as the cost of genetic testing continues to decline, we can expect to see more and more patients being tested for genetic risk factors. This will provide healthcare providers with even more data to work with, enabling them to develop even more effective treatment plans and improve patient outcomes even further.</p>"},{"location":"healthcare/#conclusion","title":"Conclusion","text":"<p>GenAI has the potential to revolutionize the healthcare industry by improving patient outcomes and streamlining healthcare operations. By analyzing genetic data, healthcare providers can identify patients who are at risk for certain diseases and develop personalized treatment plans that are tailored to their specific needs. As the field of data science continues to evolve, we can expect to see even more advanced AI algorithms and techniques being developed to analyze and interpret genetic data, enabling healthcare providers to improve patient outcomes even further.</p>"},{"location":"hessenberg/","title":"Sparse Matrices","text":""},{"location":"hessenberg/#hessenberg-matrix","title":"Hessenberg Matrix","text":"<p>A Hessenberg Matrix is a sparse matrix with the first few rows and columns having non-zero elements, and all other elements being zeroes. It is often used in computational linear algebra and has valuable applications in Machine Learning. </p>"},{"location":"hessenberg/#use-cases","title":"Use Cases","text":"<p>Some use cases include:</p> <ol> <li> <p>Solving Linear Equations: Hessenberg matrices can be used to represent systems of linear equations which are then solved efficiently using iterative methods.</p> </li> <li> <p>Eigenvalue Problems: Hessenberg matrices can be employed to solve eigenvalue problems, especially when applied to large-scale applications in Machine Learning where the size of the matrix can be significantly reduced without losing its essential characteristics.</p> </li> <li> <p>Krylov Subspace Methods: These techniques are often used for solving linear equations and eigenvalue problems. Hessenberg matrices provide a natural framework for implementing these methods, making them more efficient and accurate in many applications.</p> </li> <li> <p>Matrix Exponential Approximation: In some Machine Learning algorithms like neural networks or dynamical systems, the matrix exponential plays a crucial role. By representing the matrices as Hessenberg forms, it becomes much easier to approximate their exponentials.</p> </li> <li> <p>Sparse Matrix Techniques: Many large-scale problems in Machine Learning involve handling sparse matrices effectively. Hessenberg form is particularly useful for such scenarios since it preserves sparsity patterns and allows efficient algorithms for solving linear equations and eigenvalue problems to be implemented.</p> </li> </ol>"},{"location":"hessenberg/#advantages","title":"Advantages","text":"<p>Sparsity makes it easier to use a Hessenberg Matrix for solving linear equations by reducing the number of non-zero elements in the matrix. This results in a more efficient and faster solution process. A sparse Hessenberg Matrix contains only a few non-zero elements below its subdiagonal, which can significantly decrease storage requirements and computational complexity when compared to a dense matrix. By focusing on these non-zero elements during the solving process, one can take advantage of the structure in the matrix to solve linear equations more efficiently using algorithms designed for sparse matrices.</p>"},{"location":"labeling/","title":"Custom Labeling Platforms","text":"<p>Creating your own labeling platform and establishing direct-to-vendor agreements can significantly lower your machine learning labeling costs. In this blog post, we will discuss the benefits of these strategies and provide a step-by-step guide on how to implement them.</p>"},{"location":"labeling/#understanding-machine-learning-labeling-costs","title":"Understanding Machine Learning Labeling Costs","text":"<p>Machine learning requires large amounts of labeled data for effective training and deployment. Obtaining high-quality labels can be time-consuming and expensive, often constituting a significant portion of the overall project costs. This is where creating your own labeling platform and direct-to-vendor agreements come into play.</p>"},{"location":"labeling/#creating-your-own-labeling-platform","title":"Creating Your Own Labeling Platform","text":""},{"location":"labeling/#developing-an-in-house-labeling-platform-allows-you-to-streamline-and-automate-the-data-labeling-process-as-well-as-control-the-quality-of-labels-produced-some-benefits-include","title":"Developing an in-house labeling platform allows you to streamline and automate the data labeling process, as well as control the quality of labels produced. Some benefits include:","text":"<ul> <li>Customization: Design your platform according to your specific needs and requirements, ensuring that it is optimized for your machine learning model and data types.</li> <li>Control: Manage the entire labeling workflow, from data acquisition to annotation, and have full control over the quality of labels generated.</li> <li>Scalability: Efficiently scale your platform as your data requirements grow without being constrained by external factors or third-party limitations.</li> </ul>"},{"location":"labeling/#establishing-direct-to-vendor-agreements","title":"Establishing Direct-to-Vendor Agreements","text":""},{"location":"labeling/#direct-to-vendor-agreements-involve-partnering-with-vendors-that-specialize-in-providing-specific-types-of-annotations-instead-of-relying-on-third-party-marketplaces-this-allows-you-to","title":"Direct-to-vendor agreements involve partnering with vendors that specialize in providing specific types of annotations, instead of relying on third-party marketplaces. This allows you to:","text":"<ul> <li>Reduce Costs: By working directly with vendors, you can negotiate better prices and secure higher quality labels at a lower cost compared to marketplaces.</li> <li>Improve Efficiency: Collaborating closely with vendors enables better communication and coordination, ensuring timely delivery of high-quality annotations.</li> <li>Guarantee Compliance: Direct agreements often come with service level agreements (SLAs), ensuring the quality and compliance standards set by your organization are met.</li> </ul>"},{"location":"labeling/#implementing-the-strategies","title":"Implementing the Strategies","text":""},{"location":"labeling/#to-effectively-lower-machine-learning-labeling-costs-using-these-strategies-consider-the-following-steps","title":"To effectively lower machine learning labeling costs using these strategies, consider the following steps:","text":"<ul> <li>Assess current labeling costs and needs.</li> <li>Develop an in-house labeling platform customized for your requirements.</li> <li>Identify vendors specializing in the types of annotations required.</li> <li>Establish direct-to-vendor agreements to secure better prices and quality.</li> <li>Continuously monitor and optimize both internal and external processes to further reduce costs while maintaining quality.</li> </ul> <p>Creating your own labeling platform and establishing direct-to-vendor agreements can significantly lower machine learning labeling costs by enabling greater control, customization, scalability, and efficiency in the data annotation process. By following this guide and implementing these strategies, you can optimize your projects' cost structures and achieve better outcomes with your machine learning models.</p>"},{"location":"layernorm/","title":"Layernorm","text":"<p>Imagine you are playing with toy cars on a track. Each car is different size and speed, and  sometimes they have to go uphill or downhill. If you measure the distance each car travels,  some distances may be longer than others due to the varying terrain. Now, if you want to  compare the distances traveled by different cars, it would be unfair if some cars were  traveling much further distances than others because of the differences in size and speed.  What you need is a way to level the playing field so that all cars can travel equal distances regardless of their abilities on the track.</p> <p>This is where Layer Norm comes into play. It's like giving each car an equal starting point  on the track by scaling its distance traveled back down to a uniform level, making it easier  to compare distances across different cars. In other words, Layer Norm normalizes the data so that it has a mean of 0 and a standard deviation of 1, which ensures all features have  similar scales. This can make training a neural network more efficient and improve its  overall performance by reducing overfitting and improving generalization.</p> <p>In summary, Layer Norm is an important component of Neural Networks that helps level the  playing field for data normalization, ensuring fairness in comparisons across different  features or inputs. It's like giving each toy car on the track an equal starting point to  ensure all distances traveled are comparable, regardless of their original sizes and speeds.</p>"},{"location":"multihead_attention/","title":"MultiHead Attention","text":""},{"location":"multihead_attention/#an-extended-annotation-of-multiheadattention-from-apples-recent-whisper-implementation","title":"An extended annotation of MultiHeadAttention from Apple's recent Whisper implementation.","text":"<pre><code>class MultiHeadAttention(nn.Module):\ndef __init__(self, n_state: int, n_head: int):\nsuper().__init__()\nself.n_head = n_head\nself.query = nn.Linear(n_state, n_state)  # Define a linear layer for the query\nself.key = nn.Linear(n_state, n_state, bias=False)  # Define a linear layer for the key\nself.value = nn.Linear(n_state, n_state)  # Define a linear layer for the value\nself.out = nn.Linear(n_state, n_state)  # Define a linear layer for the output\ndef __call__(\nself,\nx,  # Input tensor\nxa=None,  # Attention mask\nmask=None,  # Attention mask\nkv_cache=None,  # Cache for key and value tensors\n):\nq = self.query(x)  # Apply the query linear layer to the input tensor\nif xa is None:  # If no attention mask is provided\nk = self.key(x)  # Apply the key linear layer to the input tensor\nv = self.value(x)  # Apply the value linear layer to the input tensor\nif kv_cache is not None:  # If a cache is provided\nk = mx.concatenate([kv_cache[0], k], axis=1)  # Concatenate the cached key tensor with the new key tensor\nv = mx.concatenate([kv_cache[1], v], axis=1)  # Concatenate the cached value tensor with the new value tensor\nelif kv_cache is None:  # If no cache is provided\nk = self.key(xa)  # Apply the key linear layer to the attention mask tensor\nv = self.value(xa)  # Apply the value linear layer to the attention mask tensor\nelse:  # If a cache is provided\nk, v = kv_cache  # Use the cached key and value tensors\nwv = self.qkv_attention(q, k, v, mask)  # Apply the attention mechanism\nreturn self.out(wv), (k, v)  # Return the output tensor and the cached key and value tensors\ndef qkv_attention(self, q, k, v, mask=None):  # Define the attention mechanism\nn_batch, n_ctx, n_state = q.shape  # Get the batch size, context length, and number of states\nscale = (n_state // self.n_head) ** -0.25  # Define a scaling factor\n\"\"\"\n                 Before the next 3 lines of code, `q`, `k`, and `v` are tensors with shape `(batch_size, sequence_length, embedding_dim)`.\n                 After the next 3 lines, `q`, `k`, and `v` are transformed into tensors with shape `(batch_size, n_head, sequence_length, embedding_dim/n_head)`.\n         The `reshape` function is used to change the shape of the tensor, and the `transpose` function is used to change the order of the dimensions. \n                 The reshaping and transposing operations are used to convert the tensors into the format required for the multi-head attention mechanism. \n                 In this format, each tensor is split into multiple heads, and each head is a tensor with shape `(batch_size, sequence_length, embedding_dim/n_head)`. \n                 This allows the attention mechanism to attend to different parts of the input sequence for each head.\n                 \"\"\"\nq = q.reshape(*q.shape[:2], self.n_head, -1).transpose(0, 2, 1, 3) * scale  # Reshape and transpose the query tensor\nk = k.reshape(*k.shape[:2], self.n_head, -1).transpose(0, 2, 3, 1) * scale  # Reshape and transpose the key tensor\nv = v.reshape(*v.shape[:2], self.n_head, -1).transpose(0, 2, 1, 3)  # Reshape and transpose the value tensor\n\"\"\"\n                 IN CASE THAT STILL IS NOT CLEAR\n                 q had shape batch x seqlen x embed_dim\n                 ....and then q became batch x seqlen x n_head x embed/n_head\n                 We HAVE to reshape efore we transpose (or else we'd move data around incorrectly\n                 n_head is ADDED to the shape, and the -1 is interpolated, meaning we have to divide the embedding size by n_head\n                 Transpose operations\n                        q is batch x seqlen x n_head x embed/n_head\n                        And goes to\n                   q = batch x n_head x seqlen x embed/n_head\n                   k = batch x n_head x embed/n_head x seqlen\n                 So when we do q @ k\n                     we have B x NHead x Seq X Emb/NHead @ B x NHead x Emb/NHead x Seq =&gt; B x Nead x Seq x Seq\n                 THIS WORKS BECAUSE OF PYTORCH'S AUTO-BROADCASTING of TENSORS\n                     (...which can result in bugs in your model code if you don't understand what is happening!)\n                 \"\"\"\nqk = q @ k  # Compute the dot product of the query and key tensors\nif mask is not None:  # If an attention mask is provided\nqk = qk + mask[:n_ctx, :n_ctx]  # Add the mask to the dot product\nqk = qk.astype(mx.float32)  # Cast the dot product to float32\nw = mx.softmax(qk, axis=-1).astype(q.dtype)  # Compute the attention weights\n# v is batch x nhead x seqlen x embed/n_head\n# w is batch x seqlen x nhead x embed/n_head\nout = (w @ v).transpose(0, 2, 1, 3)  # Apply the attention weights to the value tensor\nout = out.reshape(n_batch, n_ctx, n_state)  # Reshape the output tensor\nreturn out  # Return the output tensor\n</code></pre>"},{"location":"neurips23/","title":"Interesting Papers from NeurIPS","text":"<p>While many papers at the conference have already appeared in preprints months earlier, there are still a number of ideas that stand out as worthwhile, overlooked ideas from the past year.</p>"},{"location":"neurips23/#fine-tuning-language-models-with-just-forward-passes","title":"Fine-Tuning Language Models with Just Forward Passes","text":""},{"location":"neurips23/#abstract","title":"Abstract","text":"<p><code>Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12\u00d7 memory reduction and up to 2\u00d7 GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwis</code> https://openreview.net/forum?id=Vota6rFhBQ</p>"},{"location":"neurips23/#additive-decoders-for-latent-variables-identification-and-cartesian-product-extrapolation","title":"Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation","text":"<p>https://openreview.net/attachment?id=R6KJN1AUAR&amp;name=pdf</p>"},{"location":"neurips23/#from-pixels-to-ui-actions-learning-to-follow-instructions-via-graphical-user-interfaces","title":"From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces","text":""},{"location":"neurips23/#abstract_1","title":"Abstract","text":"<p><code>Much of the previous work towards digital agents for graphical user interfaces (GUIs) has relied on text-based representations (derived from HTML or other structured data sources), which are not always readily available. These input representations have been often coupled with custom, task-specific action spaces. This paper focuses on creating agents that interact with the digital world using the same conceptual interface that humans commonly use \u2014 via pixel-based screenshots and a generic action space corresponding to keyboard and mouse actions. Building upon recent progress in pixel-based pretraining, we show, for the first time, that it is possible for such agents to outperform human crowdworkers on the MiniWob++ benchmark of GUI-based instruction following tasks.</code> (Recent work from DeepMind and Google) https://openreview.net/attachment?id=3PjCt4kmRx&amp;name=pdf</p>"},{"location":"transformer/","title":"Toy GPT example","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n# Define the GPT-2 model\nclass GPT2Model(nn.Module):\ndef __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_heads, max_seq_length):\nsuper().__init__()\nself.embedding = nn.Embedding(vocab_size, embedding_dim)\nself.encoder = nn.TransformerEncoder(\nnn.TransformerEncoderLayer(\nembedding_dim, \nhidden_dim, \nnum_heads, \nnn.LayerNorm(embedding_dim)\n), \nnum_layers\n)\nself.fc = nn.Linear(embedding_dim, vocab_size)\nself.max_seq_length = max_seq_length\ndef forward(self, src):\nembedded = self.embedding(src)\nsrc = embedded.permute(1, 0, 2)\noutput = self.encoder(src)\noutput = self.fc(output[:, -1, :])\nreturn output\n</code></pre> <p>Here is an example of a <code>GPT2Dataset</code> class that creates the data and target tokens for the cross entropy loss:</p> <pre><code>import torch\nfrom torch.utils.data import Dataset\nclass GPT2Dataset(Dataset):\ndef __init__(self, input_ids, targets):\nself.input_ids = input_ids\nself.targets = targets\ndef __len__(self):\nreturn len(self.input_ids)\ndef __getitem__(self, idx):\nreturn self.input_ids[idx], self.targets[idx]\n</code></pre> <p>This class takes in two tensors, <code>input_ids</code> and <code>targets</code>, and stores them as attributes. The <code>__len__</code> method returns the length of the <code>input_ids</code> tensor, and the <code>__getitem__</code> method returns a tuple of the <code>input_ids</code> and <code>targets</code> tensors at the specified index. To use this class, you would first need to create the <code>input_ids</code> and <code>targets</code> tensors. You can then create an instance of the <code>GPT2Dataset</code> class and pass in these tensors as arguments: <pre><code>input_ids = torch.tensor([1, 2, 3, 4, 5])\ntargets = torch.tensor([6, 7, 8, 9, 10])\ndataset = GPT2Dataset(input_ids, targets)\n</code></pre></p> <p>You can then iterate over the dataset using a PyTorch dataloader to get the <code>input_ids</code> and <code>targets</code> for each batch: <pre><code>for input_ids, targets in dataloader:\n# Do something with the input_ids and targets\n</code></pre></p> <pre><code># Define the GPT-2 dataset\nclass GPT2Dataset(Dataset):\ndef __init__(self, text, tokenizer, max_seq_length):\nself.text = text\nself.tokenizer = tokenizer\nself.max_seq_length = max_seq_length\ndef __len__(self):\nreturn len(self.text) - self.max_seq_length\ndef __getitem__(self, idx):\ntext = self.text[idx:idx + self.max_seq_length]\ntokens = self.tokenizer.encode(text)\ntokens = tokens[:self.max_seq_length]\nreturn torch.tensor(tokens, dtype=torch.long)\n# Define the training function\ndef train(model, device, train_loader, optimizer, criterion):\nmodel.train()\ntrain_loss = 0.0\nfor batch_idx, (data, target) in enumerate(train_loader):\ndata, target = data.to(device), target.to(device)\noptimizer.zero_grad()\noutput = model(data)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step()\ntrain_loss += loss.item()\ntrain_loss /= len(train_loader.dataset)\nreturn train_loss\n# Define the evaluation function\ndef evaluate(model, device, test_loader):\nmodel.eval()\ntest_loss = 0.0\nwith torch.no_grad():\nfor data, target in test_loader:\ndata, target = data.to(device), target.to(device)\noutput = model(data)\nloss = criterion(output, target)\ntest_loss += loss.item()\ntest_loss /= len(test_loader.dataset)\nreturn test_loss\n# Define the main function\ndef main():\n# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Load the dataset\ntext = open(\"text.txt\", \"r\").read()\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntrain_dataset = GPT2Dataset(text, tokenizer, 1024)\ntest_dataset = GPT2Dataset(text, tokenizer, 1024)\n# Create the data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n# Define the model\nmodel = GPT2Model(len(tokenizer), 768, 2048, 12, 12, 1024).to(device)\n# Define the optimizer and loss function\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n# Train the model\nnum_epochs = 10\nfor epoch in range(num_epochs):\ntrain_loss = train(model, device, train_loader, optimizer, criterion)\ntest_loss = evaluate(model, device, test_loader)\nprint(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}\")\nimport torch\nfrom transformers import GPT2Tokenizer\n# Define the text to be converted\ntext = \"The quick brown fox jumps over the lazy dog.\"\n# Initialize the GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n# Convert the text into input-ids and targets\ninput_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)\ntargets = input_ids.clone().detach()\n</code></pre> <p>In this example, we first import the necessary modules and define the text that we want to convert. We then initialize the GPT-2 tokenizer using the <code>from_pretrained</code> method and pass it the name of the pre-trained model (\"gpt2\" in this case). Next, we use the <code>encode</code> method of the tokenizer to convert the text into a list of input-ids. We also set the <code>add_special_tokens</code> parameter to <code>True</code> to ensure that the input-ids include the special tokens that are used by the GPT-2 model (e.g. <code>[CLS]</code> and <code>[SEP]</code>). Finally, we create a tensor of the input-ids and a tensor of the targets. The targets are simply a copy of the input-ids, which is what we will use to fine-tune the GPT-2 model on our specific task.</p> <p>Yes, it is possible to train a GPT model to predict multiple tokens at a time. This can be achieved by adjusting the architecture of the model and the training process. One way to do this is to modify the model's output layer to predict a sequence of tokens instead of a single token. This can be done by using a recurrent neural network (RNN) or a transformer architecture with a decoder layer. The output layer would then generate a sequence of tokens one at a time, with each token being predicted based on the previous tokens in the sequence. During training, the model would be presented with a sequence of tokens as input and would be asked to predict the next token in the sequence. The loss function would be calculated based on the difference between the predicted sequence and the actual sequence of tokens. It's worth noting that training a model to predict multiple tokens at a time can be more challenging than training a model to predict a single token, as it requires the model to maintain a context and generate coherent sequences of tokens. However, with the right architecture and training process, it is definitely possible to achieve this.</p>"}]}