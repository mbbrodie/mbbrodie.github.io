<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>BERT: Bidirectional Encoder Representations from Transformers - My Docs</title>
    <link href="../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../js/jquery-3.2.1.min.js"></script>
    <script src="../js/bootstrap-3.3.7.min.js"></script>
    <script src="../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "BERT: Bidirectional Encoder Representations from Transformers", url: "#_top", children: [
              {title: "Overview", url: "#overview" },
              {title: "Key Points", url: "#key-points" },
              {title: "Discussion", url: "#discussion" },
              {title: "Self-attention", url: "#self-attention" },
              {title: "Summary", url: "#summary" },
          ]},
        ];

    </script>
    <script src="../js/base.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="../search/main.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../healthcare/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../healthcare/" class="btn btn-xs btn-link">
        GenAI in Healthcare
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href=".." class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href=".." class="btn btn-xs btn-link">
        Home
      </a>
    </div>
    
  </div>

    

    <h1 id="bert-bidirectional-encoder-representations-from-transformers">BERT: Bidirectional Encoder Representations from Transformers</h1>
<h2 id="overview">Overview</h2>
<p>BERT, or Bidirectional Encoder Representations from Transformers, is a groundbreaking NLP model developed by Google. One of the key advantages of BERT is its ability to understand context and meaning in <strong>both</strong> directions, forward and backward, through a self-supervised training process.</p>
<h2 id="key-points">Key Points</h2>
<ul>
<li>
<p>BERT achieves this advanced understanding of language by pre-training on massive amounts of unlabeled text data from sources like Wikipedia and books corpus. This allows it to capture the context and relationships between words in a sentence, rather than focusing solely on individual word meanings.</p>
</li>
<li>
<p>The model is based on <strong>Transformer</strong> architecture, which was originally proposed for machine translation tasks. BERT uses a self-attention mechanism, which allows it to consider all possible word relationships and generate highly accurate contextualized representations of words. This makes the model more robust in various language understanding tasks, such as question answering, sentiment analysis, and natural language inference.</p>
</li>
<li>
<p>BERT's performance has significantly outperformed previous state-of-the-art models on various benchmarking datasets, leading to its widespread adoption among researchers and developers. Due to this success, Google open-sourced BERT, making it accessible for further development and experimentation in the research community.</p>
</li>
<li>
<p>BERT is a powerful and innovative natural language processing model that has revolutionized the field with its ability to understand context and meaning in both forward and backward directions. Its impact is evident through its continued use by researchers even years after its initial release.</p>
</li>
</ul>
<h2 id="discussion">Discussion</h2>
<p>One of the key features of BERT is its ability to capture the context and relationships between words in a sentence, rather than focusing solely on individual word meanings. This is achieved through a process of pre-training on massive amounts of unlabeled text data from sources like Wikipedia and books corpus. By analyzing this data, BERT is able to learn the patterns and relationships between words, allowing it to generate contextualized representations of words.</p>
<p>For example, consider the sentence "The cat sat on the mat." Without any context, the word "cat" could refer to any type of feline, but with the context of the sentence, we know that it is referring to a specific cat that is sitting on a mat. BERT is able to capture this context and relationship between the words in the sentence, allowing it to generate a more accurate representation of the sentence as a whole.</p>
<p>In contrast, traditional word-based models such as word2vec or GloVe, which only consider the individual word meanings, would generate embeddings for "cat" and "mat" that do not capture the relationship between them in the context of the sentence. This can lead to less accurate predictions and poorer performance on tasks such as language translation or sentiment analysis.
BERT's ability to capture context and relationships between words is a key factor in its success and has led to significant improvements across a wide range of NLP tasks.</p>
<h2 id="self-attention">Self-attention</h2>
<p>Self-attention is a mechanism in deep learning that allows a model to attend to different parts of an input sequence when making predictions. It is a way for the model to weigh the importance of different features in the input and use that information to make more accurate predictions.</p>
<p>Intuitively, self-attention can be thought of as a way for the model to "focus" on certain parts of the input when making predictions. For example, if the input is a sequence of words, self-attention can allow the model to focus on certain words that are more important for making a prediction, rather than considering the entire sequence equally.</p>
<p>Self-attention is implemented using a set of learnable weights that are used to compute a set of attention scores. These attention scores represent the degree to which each feature in the input is important for making a prediction. The attention scores are then used to weight the input features, so that more important features are given more weight and less important features are given less weight. This is especially useful for tasks like machine translation, where the model needs to understand the context of an entire sentence rather than individual words. Self-attention has also been used in computer vision tasks, where it can help the model to focus on certain parts of an image that are more important for making a prediction.</p>
<p>Mathematically, Self-Attention can be represented as:</p>
<p>
<script type="math/tex; mode=display"> softmax(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}})</script>
</p>
<p>where Q, K, V are learnable matrices for projections of queries (Q), keys (K) and values (V). </p>
<p>The <strong>attention</strong> score between a query and each key in the sequence is calculated using a dot product, and then normalized through the softmax function to get the weight for each value vector.</p>
<p>Let's break it down step-by-step: </p>
<ol>
<li>Calculate Query Matrix:</li>
</ol>
<p>
<script type="math/tex; mode=display">\mathbf{Q} = \mathbf{x}\mathbf{W}_q</script>
</p>
<p>where x is the input sequence and W_q is a weight matrix for queries.</p>
<ol>
<li>Calculate Key Matrix:</li>
</ol>
<p>
<script type="math/tex; mode=display">\mathbf{K} = \mathbf{x}\mathbf{W}_k</script>
</p>
<p>where W_k is a weight matrix for keys.</p>
<ol>
<li>Calculate Value Matrix:</li>
</ol>
<p>
<script type="math/tex; mode=display">\mathbf{V} = \mathbf{x}\mathbf{W}_v</script>
</p>
<p>where W_v is a weight matrix for values.</p>
<p>We have now taken input x and compute 3 distinct linear transformations.</p>
<ol>
<li>Dot Product Attention:</li>
</ol>
<p>Attention scores are calculated as the dot product between Q and K, followed by a softmax function to obtain the weights for each value vector:</p>
<p>
<script type="math/tex; mode=display">\mathbf{A} = softmax(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}})</script>
</p>
<p>where d_k is the dimensionality of key vectors.</p>
<ol>
<li>Weighted Sum of Value Matrix:</li>
</ol>
<p>Finally, the output of the self-attention mechanism is obtained by taking a weighted sum of value vectors using attention scores from step 4:</p>
<p>
<script type="math/tex; mode=display">\mathbf{O} = \mathbf{V}\mathbf{A}</script>
</p>
<p>Self-Attention is a powerful technique that allows models to capture context and relationships between parts of an input sequence. The mathematical implementation involves
1. Projecting inputs as queries, keys, and values through learnable weight matrices. 
2. Dot product attention mechanism 
3. Weighted sum over value vectors.</p>
<h2 id="summary">Summary</h2>
<p>BERT is particularly useful for NLP tasks where context is crucial. For example, in question answering, BERT can understand the context of a question and generate a accurate response, even when the question is phrased in a way that differs from the original text. Similarly, in sentiment analysis tasks, BERT can accurately identify the sentiment of a sentence, even when the sentiment is expressed in a subtle or nuanced way. BERT is highly versatile and can be adapted to a wide range of NLP tasks -- and has significantly impacted the way we approach language understanding in machine learning.</p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../healthcare/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../healthcare/" class="btn btn-xs btn-link">
        GenAI in Healthcare
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href=".." class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href=".." class="btn btn-xs btn-link">
        Home
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="None">Windmill Dark</a> theme by None (noraj).</p>
</footer>

</body>
</html>